{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring reddit's comments\n",
    "\n",
    "In the following list there are reddit submission ids chosen by hand from Google search of 'Best and worst [product_family]' where product families are iphones, android phones, backpacks and liquid foundations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = ['5nrr82','chz8lg','5kl0is',\n",
    "             'jzsyko','kew1py','hvlbp2','k9btlq',\n",
    "             '8uaj1y','6wjz7o','5e7ap6','kx4bkg','ksbw5v','dhg7ej','axwymo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import praw\n",
    "import pandas as pd\n",
    "from utils import getAll\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to reddit's API to get the comments on each submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)\n",
    "\n",
    "username = config['username']\n",
    "userAgent = config['userAgent']\n",
    "clientId = config['clientId']\n",
    "clientSecret = config['clientSecret']\n",
    "password = config['password']\n",
    "r = praw.Reddit(user_agent=userAgent, client_id=clientId, client_secret=clientSecret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the comments from each submission filtering the ones that have less than 10 upvotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sub_list:\n",
    "    all_comments.append(getAll(r,sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengs = []\n",
    "for comment in all_comments:\n",
    "    lengs.append(len(comment['comments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 5, 63, 23, 2, 20, 17, 98, 69, 1, 15, 2, 7, 19]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global links\n",
    "links = 0\n",
    "def extact_links_with_text(text):\n",
    "    # Anything that isn't a square closing bracket\n",
    "    global links\n",
    "    name_regex = \"[^]]+\"\n",
    "    # http:// or https:// followed by anything but a closing paren\n",
    "    url_regex = \"http[s]?://[^)]+\"\n",
    "\n",
    "    markdown_regex = '\\[({0})]\\(\\s*({1})\\s*\\)'.format(name_regex, url_regex)\n",
    "    matches = re.findall(markdown_regex, text)\n",
    "    amazon_links = []\n",
    "    if len(matches) > 0:\n",
    "        links += 1\n",
    "    for match in matches:\n",
    "        # Replacing the link with the text\n",
    "        text = text.replace(f'[{match[0]}]({match[1]})',match[0])\n",
    "        if 'amazon.com' in match[1]:\n",
    "            print(match[1])\n",
    "            amazon_links.append(match[1])    \n",
    "    commment = {'text':text,'links':matches,'amazon_links':amazon_links}\n",
    "    return commment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global simple_links\n",
    "simple_links = 0\n",
    "def extract_links(comment):\n",
    "    global simple_links\n",
    "    url_regex = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    urls = re.findall(url_regex, comment['text'])\n",
    "    for url in urls:\n",
    "        full_url = ''.join(url)\n",
    "        comment['links'].append(full_url)\n",
    "        if 'amazon' in url[1]:\n",
    "            prod_description = url[2].split('/')[1]\n",
    "            comment['text'] = comment['text'].replace(full_url,prod_description)\n",
    "            comment['amazon_links'].append(full_url)\n",
    "        else:\n",
    "            print(''.join(url).replace('https','').replace('http',''))\n",
    "            comment['text'] = comment['text'].replace(full_url,'') \n",
    "        simple_links += 1\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link processing pipeline\n",
    "\n",
    "- First I extract the links in markdown and replace the combination of (url)[text]  with only the text.\n",
    "- Then I extract the links that are embedded en the text.\n",
    "- Whenever there's an Amazon link, I store that particular link and replace with the first part of the url.\n",
    "- I store each submission dictionary in a pickle file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.media.tumblr.com/27b571b671a3454d1edc9bc6f4ad3c32/tumblr_n3rxlsb9MM1t9sksvo1_400.gif\n",
      "www.nbcnews.com/technolog/iphone-5s-cost-make-199-iphone-5c-173-8C11256386\n",
      "https://www.amazon.com/Outdoor-Backpack-Camping-Rucksack-Shoulder/dp/B00XKUQQC8/ref=sr_1_5?ie=UTF8&qid=1530120901&sr=8-5&keywords=kaukko\n",
      "https://www.amazon.com/Herschel-Supply-Co-Little-America/dp/B0077BZW26/\n",
      "https://www.amazon.com/AmazonBasics-ZH1508073-Amazonbasics-Classic-Backpack/dp/B013TGEJEE\n",
      "www.fjallraven.co.uk/raven-20l.Its\n",
      "www.vesselworkshop.com/bike-accessories/beatnik-pnchb.This\n",
      "www.tumi.com/p/shaw-deluxe-brief-pack%C2%AE-0222389NVY2\n",
      "www.wantlesessentiels.com/us_en/shop-men#top-bread\n",
      "www.coteetciel.com/en-US/\n",
      "www.youtube.com/watch?v=ReT4YkJRewA\n",
      "gsmarena.com/results.php3?nYearMin=2018&nHeightMax=150\n",
      "www.bricklanebikes.co.uk/sugino-pista-crank-black\n"
     ]
    }
   ],
   "source": [
    "for sub in all_comments:\n",
    "    processed_comments = [extact_links_with_text(comment) for comment in sub['comments']]\n",
    "    processed_comments = [extract_links(comment) for comment in processed_comments]\n",
    "    name = '_'.join(sub['submission'].split(' ')).lower()\n",
    "    with open(f'submissions/{name}.pickle', 'wb') as handle:\n",
    "        pickle.dump(processed_comments, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
